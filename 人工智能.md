[TOC]
## Keras

序贯模型是多个网络层的线性堆叠,步骤:

1. model.add，添加层；
2. model.compile,模型训练的BP模式设置；
3. model.fit，模型训练参数设置 + 训练；
4. model.evaluate 评估模型正确率
5. model.predict 模型预测

添加层

```python
model = Sequential([
    Dense(32, units=784),#全连接层
    Activation('relu'),#输入<0输出=0，>0输出=输入
    Dense(10),
    Activation('softmax'),#多分类：k维映射为和为1的n维
])
#等同于
model = Sequential()
model.add(Dense(32, input_shape=(784,))) //input_shape是tuple类型 output_dim数设的越多越容易拟合
model.add(Activation('relu'))

model.add(Dropout(0.7)) #正确率过高用它

model.compile(loss='categorical_crossentropy', #评估预测好坏
              optimizer='adam',#惯性
              metrics=['accuracy'])
model.fit(x,y,batch_size=100,#随机100份为1组(100份合并为单矩阵利用GPU并行计算)
         nb_epoch=20)#重复20次

#添加卷积层
model.add(Convolution2D(25,3,3), #2D图，25个3*3的filter
          input_shape=(28,28,1)  #28*28的黑白图 彩色图是28*28*3
         )	# 输出的矩阵是26*26*25(28*28和3*3作内积得出26*26)
model.add(MaxPooling2D((2,2)))#输出的矩阵是13*13*25（留下2*2里的最大值）
model.add(Convolution2D(50,3,3))# 每层卷积的filter应越设越多 11*11*50
model.add(MaxPooling2D((2,2)))#5*5*50=1250 #MaxPooling时，channel数不变。
model.add(Flatten())
model.add(Dense(output_dim=100))
model.add(Activation('relu'))
model.add(Dense(output_dim=10)) #识别10个数字
model.add(Activation('softmax'))
```





### Activation

激活函数对应损失函数：softmax与categorical_crossentropy， sigmoid与binary_crossentropy

sigmoid的梯度消失：纯用sigmoid的深层网络，容易前几层梯度小，后几层梯度大。错误的抛出值得出错误的收敛。

relu:不会梯度消失。maxout的一种。

抛出值等于0就是梯度为0，后面不再更新。

### optimizer

adam比SGD(lr=0.1)收敛速度快

## CNN

适合图片识别、自己设计神经架构。

卷积层+全连接层

卷积层有多个组合(卷积+max pooling)：卷积负责识别图片局部而非整图和相同局部不同位置。max pooling负责图片像素随机抽掉变小仍能识别。

fliter：stride=1就是fliter每次遍历计算移动1列，遍历全行后下移1行重复刚才操作。filter是局部像素，用来和整图像素的各个局部作矩阵内积运算。多层卷积的filter应越设越多，高层更抽象。

矩阵数组（行数,列数）

卷积是全连接的简化版，他的神经元只连接部分输入。因为矩阵被拉直能数组了。max pooling是把feature map里最大的数字留下其它剪除组成新矩阵(阿尔法狗没有使用)。最后拉直推入全连接层。

比较两个模型时需要参数一样多。

在深度里会利用模组化。加深度比加参数效果好。

## 半监督学习

有很多无label的数据

## 无监督学习

## 线性模型

降维：多维像素图如果只是旋转角度不同，可以降为1维只记录角度。三维转二维能更好分类。

主成分分析：

线性模型可以训练单词造句预测。

# 人工智能

感知机：第二层权重依赖第一层决策结果，并输出更抽象的决策结果。
回归模型可预测连续值。
分类模型可预测离散值。
损失：如果模型的预测完全准确，则损失为零，否则损失会较大。样本点离预测线越远，损失越大。
平方损失：(y-y)^2 平方形式的时候，使用的是“最小二乘法”的思想，
wb初始值：在非凸形的有多个最低点时，选初始值很重要。线性回归无所谓选0既可。
收敛：损失不再变化或变化极其缓慢为止。可以说该模型已收敛。
训练集：梯度下降 能更新普通参数。
验证集：更新超参数如网络层数、网络节点数、迭代次数、学习率 。降低过拟合。
测试集：验证准确率。

特征工程：把原始数据转换为矢量(数组/矩阵)，数字不用转。用独热编码把唯一字符串映射为数字，1表示该字符，0是其他字符。用枚举值把多字符映射为多个数字。

二行三竖值为1矩阵：`tf.constant(1.0, shape=[2, 3])`
二行三竖值为1~7矩阵：`tf.constant( np.reshape(np.arange(1.0, 7.0, dtype=np.float32), (2, 3)) )`

数据越多，样本均值会收敛在数学期望(`p(x)*x的和`)

训练样本分成多份，训练出多个model，单次误差率不算什么，重新拆分训练样本再训练相同model，多次后看哪个model平均误差率最小。

adagrad: 学习速率/微分的均方根

特征缩放：y=b+w1x1+w2x2, 把x1x2的范围统一

微分值近似0，不一定在局部最低点，可能在高点平处

交叉熵比squareError陡峭，梯度下降更快。

逻辑回归和线性回归的区别：比如要分析性别、年龄、身高、饮食习惯对于体重的影响，如果这个体重是属于实际的重量，是连续性的数据变量，这个时候就用线性回归来做；如果将体重分类，分成了高、中、低这三种体重类型作为因变量，则采用logistic回归。 

逻辑回归：用`y=1/(1+e**(-z))`线性回归 拟合单位阶跃函数y=0/0.5/1。 用回归思想解决分类

逻辑回归和线性回归的梯度下降算法一模一样！，只是前者的y只能是0和1(估计值和真实值)，后者是任意值。

逻辑回归只能用交叉熵，不能用平方误差。

逻辑回归无法分出 数据乱摆，因为它在二维上是一条分界线。必须把数据整理成可处理的数据，通常需要行业知识。

损失函数+正则化项 可以限制模型的复杂度，进而避免过拟合 

偏导数的意义：x1-x12都对c函数有影响，想知道x5的影响，就是c对x5的偏导数，记作∂c∂x5。如果x和c之间不止1层，就要链式法则把每层与层之间的偏导数相乘。如果x的某层路径不止一个，就把每个路径的偏导数相加。

GPU并行计算要设置minibatch

梯度消失：靠近输出的权重更新快，靠近输入的权重更新慢。可能是神经层太多,而sigmoid函数又会衰减输入变化。得换ReLU

adam是梯度加上了惯性动量

正则化：剪除无用的神经元 实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的**计算量**。 对参数引入**高斯先验** 等价于L2正则化，对参数引入**拉普拉斯先验** 等价于 L1正则化 。  正则化项对应后验估计中的**先验信息**，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计 

dropout: 剪除无用的神经路径防止过拟合 训练需要，测试不需要 (w乘以(1-p)%) p是剪除概率 只在线性函数里有效。功能接近穷举测试集的平均输出。适合大数据。 0.5的时候效果最好。输入层乘以概率，是添加噪声。

稀疏性：在线性空间中，学习一个整个空间的特征集合是足够的，但是当数据分布在非线性不连续的空间中得时候，则学习局部空间的特征集合会比较好。 所以数据量小时，减小数据之间的重叠度。

#### 安装

在环境变量/pip/创建pip.ini 

```
[global]
index-url = http://mirrors.aliyun.com/pypi/simple/
[install]
trusted-host = mirrors.aliyun.com
```

更新源：python -m pip install --upgrade pip
pip install numpy
声明模块：`# Filename: support.py` 对应 `import support`

选**32位的Python**,反正64位的系统上32位的Python也能很好的运行。一个重要的原因是当前有些有用的Pyhton第三方安装包不支持64位系统比如numpy、scipy等。

**语法**：

```
a, b = 0, 1  等价js于 [a, b] = [0, 1] 
lambda x:x+1 等价js于 x => x+1
```

for v in [] 
for i in range(10):
else:  //没有break的进到这
type()

格式化：

1. `'{:.1f}'.format()` 保留1位小数
2. {:b}转二进制 {:d}十进制 {:o}八进制 {:x}十六进制
3. {:,}千分位逗号
4. {:>8}右对齐，字符总宽设8，:>之间的为填充符。^、<、>分别是居中、左对齐、右对齐
5. `'{a},{b}'.format(a=18,b='k')` 等同js于 `${a},${b}` 可传入对象，可用索引{0}表示format的0位参。

#### numpy

sum() axis=0列和 axis=1行和
np.log 以e为底的对数

#### pandas

特点：高性能数组计算以及电子表格和关系型数据库的数据处理，适用于数据分析
`import pandas as pd`
pd.series([1,2]) 一维的单列(类似数组)

1. +-*%可直接作用于series列
2. 多个列可合并：(s1>5)&(s2<10)
3. apply() 类似js的fliter

pd.DataFrame({'a':series1,'b':series2}) 二维表结构 

1. describe()汇总数据集分布的中心趋势，离散度和形状
2. reindex([2, 0, 1])按指定索引排序 乱序排列：cities.reindex(np.random.permutation(cities.index))
3. `dataframe[["total"]]`和`dataframe["total"]`不一样，feature列用前种。
   pd.read_csv("./a.csv", sep=",") 导入表为DataFrame

#### keras

安装依赖：
numpy scipy tensorflow 
Microsoft Visual C++ 2015 Redistributable Update 3

修改keras.json的backend为tensorflow
import os
os.environ['KERAS_BACKEND']=tensorflow

Sequential 顺序神经乘模型
Dense 全连接层  input_dim=28*28 28维，下层不需要input了， units神经元数量/输出维度 activation激活函数

matplotlib.pyplot 可视化模块
model.add() 加神经层
model.compile() 
  loss='mse' 二次方误差函数 其它有交叉熵
  optimizer='sgd' 优化器之乱序

model.train_on_batch([],[]) 循环手动训练 

model.fit 自动训练

model.evaluate([],[],batch_size=100) 测试100个

model.predict 预测

model.layers[0].get_weights() 返回w b

优化器的SGD就是梯度下降



#### tensorflow

安装：

```
win10_64不支持32位python
对于tensorflow 1.9来说，就得是CUDA 9.0. bin加环境变量
把 cuDNN 7.0.5的.dll放在cuda的bin文件夹下。
pip3 install --upgrade tensorflow-gpu //显卡需支持CUDA
```



定义特征列tf.feature_column.numeric_column("total_rooms")
配置线性回归梯度下降
my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
linear_regressor = tf.estimator.LinearRegressor(
​    feature_columns=feature_columns,
​    optimizer=my_optimizer
)

估值低确定性高的成长股 > 盈利确定性高的价值股



# 周志华-机器学习

### 术语(数学)

假如某人的支持率为55%，而置信水平0.95上的置信区间是（50%,60%）。

置信区间：在某一置信水平下，样本统计值与总体参数值间误差范围。

置信极限：区间的两端。

置信水平：总体参数值落在样本统计值某一区内的概率。

参数估计：通过样本估计总体未知参数。包含矩法估计、最小二乘估计、似然估计、贝叶斯估计。包含点估计与区间估计。

### 术语

样本/示例：就是特征向量。

样本空间/属性空间/输入空间：属性张成的空间。

属性值：样本的某个属性取值。

样例：有标记的样本。

标记空间/输出空间：样例的集合。

分类：预测离散值。

回归：预测连续值。

簇(cluster)：训练集的每个分组。

归纳学习：包括监督、无监督、符号主义(决策树)、连接主义(神经网络/深度学习)、统计学习(向量机/核方法)。

监督学习：分类、回归。

无监督学习：聚类。

泛化能力：模型预测新样本的能力。

独立同分布：	每个样本都是独立从分布上采集的。

假设空间：所有映射模型

版本空间：与训练集一致的假设集合。

归纳偏好：存在多条曲线可拟合训练集，每种算法都有对具体问题的偏好。在其他问题上和胡猜概率一样。

奥卡姆剃刀：若有多条曲线可拟合，选最简单最平滑的。

精度：精度 = 1 - 分类错误率

分类错误率 = 错误数 / 测试集数

误差：预测输出和实际输出的差异

分层采样：按数据分布对样本采样

验证集：和训练集测试集不相交，训练模型的超参数(多项式次数、学习速率)。

### 知识点

统计学通过机器学习影响数据挖掘。

欠拟合好解决，过拟合是机器学习的主要障碍。

留出法：n次随机划分训练集和测试集，且注意数据分布一致。测试集小，评估结果方差大，测试集大，评估结果偏差大。

留一法/k折交叉验证法：k次把数据集集划分为k组，依序把每组作为测试集，k-1的并集作训练集。k次训练返回的均值。计算量随数据集增大而增大。

自助采样法：从数量为m的数据集D中随机放回抽取样本到D‘内，抽取m次作为训练集。测试集用D\D’(D中除去所有属于D’的元素)。适合数据量过小且难以划分训练测试集时，会改变数据分布。

##### 评估

均方误差：用于回归问题的性能评估。(图解：密度函数p(x)是高，dx是宽，p(x)dx是连续随机变量在某区间取值的概率)

![均方误差.png](https://github.com/chenjun-110/note/blob/master/image/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE.png?raw=true)

##### 性能度量

查准率 = 预测真正例/(预测真正例+预测假正例) 用户更感兴趣的内容 

查全率 = 预测真正例/(预测真正例+预测真反例) 准确率更高

平衡点：当查准率等于查全率时。AUG是ROC曲线的面积。

宏查准率/宏查全率是n个2分类混淆矩阵的平均值。微查准率/微查全率是n个2分类混淆矩阵的对应元素的平均。

代价敏感错误率：反应预测错误对后续的代价

##### 比较模型

假设检验：根据测试错误率估推出泛化错误率的分布

T分布：用来估计方差未知且呈正态分布的总体均值。已知方差直接用正态分布估计。当自由度df=∞时，t分布等于标准正态分布。

交叉验证T检验 McNemar检验

Friedman检验和Nemenyi后续检验能在多个数据集对多个算法比较。

##### 泛化误差

泛化误差：偏差+方差+噪声。

偏差：算法的预测和真实的偏离程度，体现了算法的拟合能力。

方差：训练集变动导致学习性能的变化，体现了算法的抗干扰能力。

噪声：体现了问题的难度。

偏差方差仅适合均方误差的回归分析。

### 线性模型

f(x) = wx + b

非线性模型可在线性模型基础上通过层级结构或高维映射得到。

#### 线性回归

数据转化：1. 西瓜南瓜黄瓜可转为k维向量-(0,0,1),(0,1,0),(1,0,0)   2. 高中低可转为连续值-{1.0,0.5,0.0}

最小二乘法：均方误差最小化。所有样本到直线欧式距离和最小

凸函数：E(w,b)是关于w、b的凸函数，当然关于w和b对导数为0时，得到w和b的最优解。

几率=正例/反例 对几率取对数。

极大似然法：每个样本属于真实标记的概率越大越好。

### 聚类

#### Kmean

Kmean:先随机取n个点得出n个均值，遍历所有点进行分簇，得出新的n个均值，重复该过程直到均值不变。

前提： 到底分几类很重要，不要过度解释聚类结果。

```python
from sklearn.cluster import KMeans
km=KMeans(n_clusters=num)
lable=km.fit_predict(data)

k_means = cluster.KMeans(n_clusters=3)
k_means.fit(data) #data格式是二维[[1,2],[3,4]] 如果特征只有1种可降成2维1项(单列)
print("聚类后data的簇标签",k_means.labels_)
print("聚类后data的簇中心点",km.cluster_centers_)

# np.choose(labels, values)可以根据顺序把标签替换为中心点
# 通过.reshape((-1, 1))降维的可用.shape复原

```

