## Keras

序贯模型是多个网络层的线性堆叠,步骤:

1. model.add，添加层；
2. model.compile,模型训练的BP模式设置；
3. model.fit，模型训练参数设置 + 训练；
4. model.evaluate 评估模型正确率
5. model.predict 模型预测

添加层

```python
model = Sequential([
    Dense(32, units=784),#全连接层
    Activation('relu'),#输入<0输出=0，>0输出=输入
    Dense(10),
    Activation('softmax'),#多分类：k维映射为和为1的n维
])
#等同于
model = Sequential()
model.add(Dense(32, input_shape=(784,))) //input_shape是tuple类型
model.add(Activation('relu'))

model.compile(loss='categorical_crossentropy', #评估预测好坏
              optimizer='adam',#惯性
              metrics=['accuracy'])
model.fit(x,y,batch_size=100,#随机100份为1组(100份合并为单矩阵利用GPU并行计算)
         nb_epoch=20)#重复20次
```





#### Activation

激活函数对应损失函数：softmax与categorical_crossentropy， sigmoid与binary_crossentropy

sigmoid的梯度消失：纯用sigmoid的深层网络，容易前几层梯度小，后几层梯度大。错误的抛出值得出错误的收敛。

relu:不会梯度消失。maxout的一种。

抛出值等于0就是梯度为0，后面不再更新。



#### 人工智能

感知机：第二层权重依赖第一层决策结果，并输出更抽象的决策结果。
回归模型可预测连续值。
分类模型可预测离散值。
损失：如果模型的预测完全准确，则损失为零，否则损失会较大。样本点离预测线越远，损失越大。
平方损失：(y-y)^2 平方形式的时候，使用的是“最小二乘法”的思想，
wb初始值：在非凸形的有多个最低点时，选初始值很重要。线性回归无所谓选0既可。
收敛：损失不再变化或变化极其缓慢为止。可以说该模型已收敛。
训练集：梯度下降 能更新普通参数。
验证集：更新超参数如网络层数、网络节点数、迭代次数、学习率 。降低过拟合。
测试集：验证准确率。

特征工程：把原始数据转换为矢量(数组/矩阵)，数字不用转。用独热编码把唯一字符串映射为数字，1表示该字符，0是其他字符。用枚举值把多字符映射为多个数字。

二行三竖值为1矩阵：`tf.constant(1.0, shape=[2, 3])`
二行三竖值为1~7矩阵：`tf.constant( np.reshape(np.arange(1.0, 7.0, dtype=np.float32), (2, 3)) )`

数据越多，样本均值会收敛在数学期望(`p(x)*x的和`)

训练样本分成多份，训练出多个model，单次误差率不算什么，重新拆分训练样本再训练相同model，多次后看哪个model平均误差率最小。

adagrad: 学习速率/微分的均方根

特征缩放：y=b+w1x1+w2x2, 把x1x2的范围统一

微分值近似0，不一定在局部最低点，可能在高点平处

交叉熵比squareError陡峭，梯度下降更快。

逻辑回归和线性回归的区别：比如要分析性别、年龄、身高、饮食习惯对于体重的影响，如果这个体重是属于实际的重量，是连续性的数据变量，这个时候就用线性回归来做；如果将体重分类，分成了高、中、低这三种体重类型作为因变量，则采用logistic回归。 

逻辑回归：用`y=1/(1+e**(-z))`线性回归 拟合单位阶跃函数y=0/0.5/1。 用回归思想解决分类

逻辑回归和线性回归的梯度下降算法一模一样！，只是前者的y只能是0和1(估计值和真实值)，后者是任意值。

逻辑回归只能用交叉熵，不能用平方误差。

逻辑回归无法分出 数据乱摆，因为它在二维上是一条分界线。必须把数据整理成可处理的数据，通常需要行业知识。

损失函数+正则化项 可以限制模型的复杂度，进而避免过拟合 

偏导数的意义：x1-x12都对c函数有影响，想知道x5的影响，就是c对x5的偏导数，记作∂c∂x5。如果x和c之间不止1层，就要链式法则把每层与层之间的偏导数相乘。如果x的某层路径不止一个，就把每个路径的偏导数相加。

GPU并行计算要设置minibatch

梯度消失：靠近输出的权重更新快，靠近输入的权重更新慢。可能是神经层太多,而sigmoid函数又会衰减输入变化。得换ReLU

adam是梯度加上了惯性动量

正则化：剪除无用的神经元 实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的**计算量**。 对参数引入 **高斯先验** 等价于L2正则化，对参数引入 **拉普拉斯先验** 等价于 L1正则化 。  正则化项对应后验估计中的 **先验信息**，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计 

dropout: 剪除无用的神经路径防止过拟合 训练需要，测试不需要 (w乘以(1-p)%) p是剪除概率 只在线性函数里有效。功能接近穷举测试集的平均输出。适合大数据。 0.5的时候效果最好。输入层乘以概率，是添加噪声。

稀疏性：在线性空间中，学习一个整个空间的特征集合是足够的，但是当数据分布在非线性不连续的空间中得时候，则学习局部空间的特征集合会比较好。 所以数据量小时，减小数据之间的重叠度。

#### 安装

在环境变量/pip/创建pip.ini 

```
[global]
index-url = http://mirrors.aliyun.com/pypi/simple/
[install]
trusted-host = mirrors.aliyun.com
```

更新源：python -m pip install --upgrade pip
pip install numpy
声明模块：`# Filename: support.py` 对应 `import support`

选**32位的Python**,反正64位的系统上32位的Python也能很好的运行。一个重要的原因是当前有些有用的Pyhton第三方安装包不支持64位系统比如numpy、scipy等。

**语法**：

```
a, b = 0, 1  等价js于 [a, b] = [0, 1] 
lambda x:x+1 等价js于 x => x+1
```

for v in [] 
for i in range(10):
else:  //没有break的进到这
type()

格式化：

1. `'{:.1f}'.format()` 保留1位小数
2. {:b}转二进制 {:d}十进制 {:o}八进制 {:x}十六进制
3. {:,}千分位逗号
4. {:>8}右对齐，字符总宽设8，:>之间的为填充符。^、<、>分别是居中、左对齐、右对齐
5. `'{a},{b}'.format(a=18,b='k')` 等同js于 `${a},${b}` 可传入对象，可用索引{0}表示format的0位参。

#### numpy

sum() axis=0列和 axis=1行和
np.log 以e为底的对数

#### pandas

特点：高性能数组计算以及电子表格和关系型数据库的数据处理，适用于数据分析
`import pandas as pd`
pd.series([1,2]) 一维的单列(类似数组)

1. +-*%可直接作用于series列
2. 多个列可合并：(s1>5)&(s2<10)
3. apply() 类似js的fliter

pd.DataFrame({'a':series1,'b':series2}) 二维表结构 

1. describe()汇总数据集分布的中心趋势，离散度和形状
2. reindex([2, 0, 1])按指定索引排序 乱序排列：cities.reindex(np.random.permutation(cities.index))
3. `dataframe[["total"]]`和`dataframe["total"]`不一样，feature列用前种。
   pd.read_csv("./a.csv", sep=",") 导入表为DataFrame

#### keras

安装依赖：
numpy scipy tensorflow 
Microsoft Visual C++ 2015 Redistributable Update 3

修改keras.json的backend为tensorflow
import os
os.environ['KERAS_BACKEND']=tensorflow

Sequential 顺序神经乘模型
Dense 全连接层  input_dim=28*28 28维，下层不需要input了， units神经元数量/输出维度 activation激活函数

matplotlib.pyplot 可视化模块
model.add() 加神经层
model.compile() 
  loss='mse' 二次方误差函数 其它有交叉熵
  optimizer='sgd' 优化器之乱序

model.train_on_batch([],[]) 循环手动训练 

model.fit 自动训练

model.evaluate([],[],batch_size=100) 测试100个

model.predict 预测

model.layers[0].get_weights() 返回w b

优化器的SGD就是梯度下降



#### tensorflow

安装：

```
win10_64不支持32位python
对于tensorflow 1.9来说，就得是CUDA 9.0. bin加环境变量
把 cuDNN 7.0.5的.dll放在cuda的bin文件夹下。
pip3 install --upgrade tensorflow-gpu //显卡需支持CUDA
```



定义特征列tf.feature_column.numeric_column("total_rooms")
配置线性回归梯度下降
my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
linear_regressor = tf.estimator.LinearRegressor(
​    feature_columns=feature_columns,
​    optimizer=my_optimizer
)